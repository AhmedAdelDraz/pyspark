{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Demand Forecasting Using PySpark**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective:\n",
    "- generate a large number of fine-grained forecasts at the store-item level in an efficient manner leveraging the distributed computational power. which by the way I don't have Xd.\n",
    "- We'll use a dataset from Kaggle. **Store Item Demand Forecasting Challenge**<br>\n",
    "https://www.kaggle.com/c/demand-forecasting-kernels-only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T06:19:31.754790Z",
     "iopub.status.busy": "2022-01-30T06:19:31.754368Z",
     "iopub.status.idle": "2022-01-30T06:20:25.310549Z",
     "shell.execute_reply": "2022-01-30T06:20:25.309541Z",
     "shell.execute_reply.started": "2022-01-30T06:19:31.754688Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q findspark\n",
    "!pip install -q pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create a spark session and import the required libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T06:20:25.313400Z",
     "iopub.status.busy": "2022-01-30T06:20:25.313082Z",
     "iopub.status.idle": "2022-01-30T06:20:32.277272Z",
     "shell.execute_reply": "2022-01-30T06:20:32.276434Z",
     "shell.execute_reply.started": "2022-01-30T06:20:25.313357Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "from fbprophet import Prophet\n",
    "from fbprophet.plot import plot_plotly\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext as sc\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"demand_forecast\").getOrCreate()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "# disable informational messages from fbprophet\n",
    "logging.getLogger('py4j').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T06:20:32.280214Z",
     "iopub.status.busy": "2022-01-30T06:20:32.279267Z",
     "iopub.status.idle": "2022-01-30T06:20:34.571278Z",
     "shell.execute_reply": "2022-01-30T06:20:34.570303Z",
     "shell.execute_reply.started": "2022-01-30T06:20:32.280163Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# structure of the training data set\n",
    "train_schema = StructType([\n",
    "  StructField('date', DateType()),\n",
    "  StructField('store', IntegerType()),\n",
    "  StructField('item', IntegerType()),\n",
    "  StructField('sales', IntegerType())\n",
    "  ])\n",
    "\n",
    "# read the training file into a dataframe\n",
    "train = spark.read.csv(\n",
    "  '../input/demand-forecasting-kernels-only/train.csv', \n",
    "  header=True, schema = train_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T06:20:34.572974Z",
     "iopub.status.busy": "2022-01-30T06:20:34.572541Z",
     "iopub.status.idle": "2022-01-30T06:20:36.864701Z",
     "shell.execute_reply": "2022-01-30T06:20:36.863820Z",
     "shell.execute_reply.started": "2022-01-30T06:20:34.572927Z"
    }
   },
   "outputs": [],
   "source": [
    "train.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Print the schema**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T06:20:36.867122Z",
     "iopub.status.busy": "2022-01-30T06:20:36.866783Z",
     "iopub.status.idle": "2022-01-30T06:20:36.882183Z",
     "shell.execute_reply": "2022-01-30T06:20:36.881526Z",
     "shell.execute_reply.started": "2022-01-30T06:20:36.867078Z"
    }
   },
   "outputs": [],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T06:20:36.883306Z",
     "iopub.status.busy": "2022-01-30T06:20:36.882974Z",
     "iopub.status.idle": "2022-01-30T06:20:36.947458Z",
     "shell.execute_reply": "2022-01-30T06:20:36.946775Z",
     "shell.execute_reply.started": "2022-01-30T06:20:36.883262Z"
    }
   },
   "outputs": [],
   "source": [
    "# make the dataframe queriable as a temporary view\n",
    "train.createOrReplaceTempView('train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Exploration**\n",
    "**Exploring general trends and seasonality**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T06:20:36.948575Z",
     "iopub.status.busy": "2022-01-30T06:20:36.948368Z",
     "iopub.status.idle": "2022-01-30T06:20:41.034575Z",
     "shell.execute_reply": "2022-01-30T06:20:41.033697Z",
     "shell.execute_reply.started": "2022-01-30T06:20:36.948549Z"
    }
   },
   "outputs": [],
   "source": [
    "yearly_sales = spark.sql('''\n",
    "SELECT\n",
    "  year(date) as year, \n",
    "  sum(sales) as sales\n",
    "FROM train\n",
    "GROUP BY year(date)\n",
    "ORDER BY year;\n",
    "''').toPandas()\n",
    "\n",
    "plt.plot(yearly_sales['year'],yearly_sales['sales'])\n",
    "plt.title('');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T06:20:41.036306Z",
     "iopub.status.busy": "2022-01-30T06:20:41.035994Z",
     "iopub.status.idle": "2022-01-30T06:20:44.477751Z",
     "shell.execute_reply": "2022-01-30T06:20:44.476958Z",
     "shell.execute_reply.started": "2022-01-30T06:20:41.036266Z"
    }
   },
   "outputs": [],
   "source": [
    "monthly_sales = spark.sql('''\n",
    "SELECT \n",
    "  TRUNC(date, 'MM') as month,\n",
    "  SUM(sales) as sales\n",
    "FROM train\n",
    "GROUP BY TRUNC(date, 'MM')\n",
    "ORDER BY month;''').toPandas()\n",
    "\n",
    "plt.plot(monthly_sales['month'],monthly_sales['sales']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregating the data at a weekday level, a pronounced weekly seasonal pattern is observed with a peak on Sunday (weekday 0), a hard drop on Monday (weekday 1) and then a steady pickup over the week heading back to the Sunday high. This pattern seems to be pretty stable across the five years of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T06:20:44.482831Z",
     "iopub.status.busy": "2022-01-30T06:20:44.482158Z",
     "iopub.status.idle": "2022-01-30T06:20:47.686964Z",
     "shell.execute_reply": "2022-01-30T06:20:47.686346Z",
     "shell.execute_reply.started": "2022-01-30T06:20:44.482784Z"
    }
   },
   "outputs": [],
   "source": [
    "weekly_sales = spark.sql('''\n",
    "SELECT\n",
    "  YEAR(date) as year,\n",
    "  CAST(DATE_FORMAT(date, 'F') as Integer) % 7 as weekday,\n",
    "  AVG(sales) as sales\n",
    "FROM (\n",
    "  SELECT \n",
    "    date,\n",
    "    SUM(sales) as sales\n",
    "  FROM train\n",
    "  GROUP BY date\n",
    " ) x\n",
    "GROUP BY year, CAST(DATE_FORMAT(date, 'F') as Integer)\n",
    "ORDER BY year, weekday;\n",
    "''').toPandas()\n",
    "sns.lineplot(x=weekly_sales['weekday'],y=weekly_sales['sales'],hue=weekly_sales['year']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Build a Forecast**\n",
    "* We will build a forcasting model using fbprophet for single item-store for illustraintion. \n",
    "* Based on our review of the data, it looks like we should set our overall growth pattern to linear and enable the evaluation of weekly and yearly seasonal patterns. We might also wish to set our seasonality mode to multiplicative as the seasonal pattern seems to grow with overall growth in sales.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T06:21:34.195743Z",
     "iopub.status.busy": "2022-01-30T06:21:34.194853Z",
     "iopub.status.idle": "2022-01-30T06:21:36.919529Z",
     "shell.execute_reply": "2022-01-30T06:21:36.918920Z",
     "shell.execute_reply.started": "2022-01-30T06:21:34.195696Z"
    }
   },
   "outputs": [],
   "source": [
    "# query to aggregate data to date (ds) level\n",
    "sql_statement = '''\n",
    "  SELECT\n",
    "    CAST(date as date) as ds,\n",
    "    sales as y\n",
    "  FROM train\n",
    "  WHERE store=1 AND item=1\n",
    "  ORDER BY ds\n",
    "  '''\n",
    "\n",
    "# assemble dataset in Pandas dataframe\n",
    "history_pd = spark.sql(sql_statement).toPandas()\n",
    "\n",
    "# drop any missing records\n",
    "history_pd = history_pd.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T06:22:25.358042Z",
     "iopub.status.busy": "2022-01-30T06:22:25.357749Z",
     "iopub.status.idle": "2022-01-30T06:22:27.149563Z",
     "shell.execute_reply": "2022-01-30T06:22:27.149034Z",
     "shell.execute_reply.started": "2022-01-30T06:22:25.358015Z"
    }
   },
   "outputs": [],
   "source": [
    "# set model parameters\n",
    "model = Prophet(\n",
    "  interval_width=0.95,\n",
    "  growth='linear',\n",
    "  daily_seasonality=False,\n",
    "  weekly_seasonality=True,\n",
    "  yearly_seasonality=True,\n",
    "  seasonality_mode='multiplicative'\n",
    "  )\n",
    "\n",
    "# fit the model to historical data\n",
    "model.fit(history_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T06:22:29.504724Z",
     "iopub.status.busy": "2022-01-30T06:22:29.503911Z",
     "iopub.status.idle": "2022-01-30T06:22:33.260221Z",
     "shell.execute_reply": "2022-01-30T06:22:33.259250Z",
     "shell.execute_reply.started": "2022-01-30T06:22:29.504678Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a dataset including both historical dates & 90-days beyond the last available date\n",
    "future_pd = model.make_future_dataframe(\n",
    "  periods=90, \n",
    "  freq='d', \n",
    "  include_history=True\n",
    "  )\n",
    "\n",
    "# predict over the dataset\n",
    "forecast_pd = model.predict(future_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T06:22:43.006815Z",
     "iopub.status.busy": "2022-01-30T06:22:43.006091Z",
     "iopub.status.idle": "2022-01-30T06:22:44.206297Z",
     "shell.execute_reply": "2022-01-30T06:22:44.205352Z",
     "shell.execute_reply.started": "2022-01-30T06:22:43.006770Z"
    }
   },
   "outputs": [],
   "source": [
    "trends_fig = model.plot_components(forecast_pd);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T06:22:47.441974Z",
     "iopub.status.busy": "2022-01-30T06:22:47.441674Z",
     "iopub.status.idle": "2022-01-30T06:22:48.003626Z",
     "shell.execute_reply": "2022-01-30T06:22:48.002739Z",
     "shell.execute_reply.started": "2022-01-30T06:22:47.441930Z"
    }
   },
   "outputs": [],
   "source": [
    "predict_fig = model.plot( forecast_pd, xlabel='date', ylabel='sales')\n",
    "\n",
    "# adjust figure to display dates from last year + the 90 day forecast\n",
    "xlim = predict_fig.axes[0].get_xlim()\n",
    "new_xlim = ( xlim[1]-(180.0+365.0), xlim[1]-90.0)\n",
    "predict_fig.axes[0].set_xlim(new_xlim);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T06:22:54.396482Z",
     "iopub.status.busy": "2022-01-30T06:22:54.395941Z",
     "iopub.status.idle": "2022-01-30T06:22:54.585957Z",
     "shell.execute_reply": "2022-01-30T06:22:54.585095Z",
     "shell.execute_reply.started": "2022-01-30T06:22:54.396438Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from datetime import date,datetime\n",
    "\n",
    "# get historical actuals & predictions for comparison\n",
    "actuals_pd = history_pd[ history_pd['ds'] < pd.to_datetime(date(2018, 1, 1)) ]['y']\n",
    "predicted_pd = forecast_pd[ forecast_pd['ds'] < pd.to_datetime(date(2018, 1, 1)) ]['yhat']\n",
    "\n",
    "# calculate evaluation metrics\n",
    "mae = mean_absolute_error(actuals_pd, predicted_pd)\n",
    "mse = mean_squared_error(actuals_pd, predicted_pd)\n",
    "rmse = sqrt(mse)\n",
    "\n",
    "# print metrics to the screen\n",
    "print( '\\n'.join(['MAE: {0}', 'MSE: {1}', 'RMSE: {2}']).format(mae, mse, rmse) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Scaling Model Training & Forecasting**\n",
    "\n",
    "* We will start by assembling sales data at the store-item-date level of granularity.<br>\n",
    "\n",
    "* To train the model and generate a forecast we will leverage a Pandas user-defined function (UDF). We will define this function to receive a subset of data organized around a store and item combination.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T06:22:58.797521Z",
     "iopub.status.busy": "2022-01-30T06:22:58.797266Z",
     "iopub.status.idle": "2022-01-30T06:22:58.895115Z",
     "shell.execute_reply": "2022-01-30T06:22:58.893736Z",
     "shell.execute_reply.started": "2022-01-30T06:22:58.797495Z"
    }
   },
   "outputs": [],
   "source": [
    "sql_statement = '''\n",
    "  SELECT\n",
    "    store,\n",
    "    item,\n",
    "    CAST(date as date) as ds,\n",
    "    SUM(sales) as y\n",
    "  FROM train\n",
    "  GROUP BY store, item, ds\n",
    "  ORDER BY store, item, ds\n",
    "  '''\n",
    "\n",
    "store_item_history = (\n",
    "  spark\n",
    "    .sql(sql_statement)\n",
    "#     .repartition(2, ['store', 'item'])\n",
    "  ).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T06:23:01.924539Z",
     "iopub.status.busy": "2022-01-30T06:23:01.923740Z",
     "iopub.status.idle": "2022-01-30T06:23:01.930971Z",
     "shell.execute_reply": "2022-01-30T06:23:01.930264Z",
     "shell.execute_reply.started": "2022-01-30T06:23:01.924483Z"
    }
   },
   "outputs": [],
   "source": [
    "result_schema =StructType([\n",
    "  StructField('ds',DateType()),\n",
    "  StructField('store',IntegerType()),\n",
    "  StructField('item',IntegerType()),\n",
    "  StructField('y',FloatType()),\n",
    "  StructField('yhat',FloatType()),\n",
    "  StructField('yhat_upper',FloatType()),\n",
    "  StructField('yhat_lower',FloatType())\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T06:23:03.300742Z",
     "iopub.status.busy": "2022-01-30T06:23:03.300442Z",
     "iopub.status.idle": "2022-01-30T06:23:03.317338Z",
     "shell.execute_reply": "2022-01-30T06:23:03.316388Z",
     "shell.execute_reply.started": "2022-01-30T06:23:03.300709Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "@pandas_udf( result_schema, PandasUDFType.GROUPED_MAP )\n",
    "def forecast_store_item( history_pd ):\n",
    "    # TRAIN MODEL AS BEFORE\n",
    "    # --------------------------------------\n",
    "    # remove missing values (more likely at day-store-item level)\n",
    "    history_pd = history_pd.dropna()\n",
    "\n",
    "    # configure the model\n",
    "    model = Prophet(\n",
    "        interval_width=0.95,\n",
    "        growth='linear',\n",
    "        daily_seasonality=False,\n",
    "        weekly_seasonality=True,\n",
    "        yearly_seasonality=True,\n",
    "        seasonality_mode='multiplicative')\n",
    "\n",
    "    # train the model\n",
    "    model.fit(history_pd)\n",
    "    # --------------------------------------\n",
    "\n",
    "    # BUILD FORECAST AS BEFORE\n",
    "    # --------------------------------------\n",
    "    # make predictions\n",
    "    future_pd = model.make_future_dataframe(\n",
    "        periods=90, \n",
    "        freq='d', \n",
    "        include_history=True)\n",
    "    forecast_pd = model.predict( future_pd )  \n",
    "    # --------------------------------------\n",
    "\n",
    "    # ASSEMBLE EXPECTED RESULT SET\n",
    "    # --------------------------------------\n",
    "    # get relevant fields from forecast\n",
    "    f_pd = forecast_pd[ ['ds','yhat', 'yhat_upper', 'yhat_lower'] ].set_index('ds')\n",
    "\n",
    "    # get relevant fields from history\n",
    "    h_pd = history_pd[['ds','store','item','y']].set_index('ds')\n",
    "\n",
    "    # join history and forecast\n",
    "    results_pd = f_pd.join( h_pd, how='left' )\n",
    "    results_pd.reset_index(level=0, inplace=True)\n",
    "\n",
    "    # get store & item from incoming data set\n",
    "    results_pd['store'] = history_pd['store'].iloc[0]\n",
    "    results_pd['item'] = history_pd['item'].iloc[0]\n",
    "    # --------------------------------------\n",
    "\n",
    "    # return expected dataset\n",
    "    return results_pd[ ['ds', 'store', 'item', 'y', 'yhat', 'yhat_upper', 'yhat_lower'] ]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T06:23:10.269817Z",
     "iopub.status.busy": "2022-01-30T06:23:10.269518Z",
     "iopub.status.idle": "2022-01-30T06:23:10.387386Z",
     "shell.execute_reply": "2022-01-30T06:23:10.386372Z",
     "shell.execute_reply.started": "2022-01-30T06:23:10.269785Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date\n",
    "\n",
    "results = (\n",
    "  store_item_history\n",
    "    .groupBy('store', 'item')\n",
    "    .apply(forecast_store_item)\n",
    "    .withColumn('training_date', current_date() )\n",
    "    )\n",
    "\n",
    "results.createOrReplaceTempView('new_forecasts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T06:23:11.247208Z",
     "iopub.status.busy": "2022-01-30T06:23:11.246877Z",
     "iopub.status.idle": "2022-01-30T06:23:11.267754Z",
     "shell.execute_reply": "2022-01-30T06:23:11.266818Z",
     "shell.execute_reply.started": "2022-01-30T06:23:11.247176Z"
    }
   },
   "outputs": [],
   "source": [
    "forecast = spark.sql('''\n",
    "select \n",
    "  ds as date,\n",
    "  store,\n",
    "  item,\n",
    "  y as sales,\n",
    "  yhat as sales_predicted,\n",
    "  yhat_upper as sales_predicted_upper,\n",
    "  yhat_lower as sales_predicted_lower,\n",
    "  training_date\n",
    "from new_forecasts;''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Print Forecasting Schema**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T06:23:13.427102Z",
     "iopub.status.busy": "2022-01-30T06:23:13.426769Z",
     "iopub.status.idle": "2022-01-30T06:23:13.432484Z",
     "shell.execute_reply": "2022-01-30T06:23:13.431613Z",
     "shell.execute_reply.started": "2022-01-30T06:23:13.427060Z"
    }
   },
   "outputs": [],
   "source": [
    "forecast.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluation of Forecasting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T06:23:16.880217Z",
     "iopub.status.busy": "2022-01-30T06:23:16.879888Z",
     "iopub.status.idle": "2022-01-30T06:23:16.885726Z",
     "shell.execute_reply": "2022-01-30T06:23:16.884859Z",
     "shell.execute_reply.started": "2022-01-30T06:23:16.880184Z"
    }
   },
   "outputs": [],
   "source": [
    "# schema of expected result set\n",
    "eval_schema =StructType([\n",
    "  StructField('training_date', DateType()),\n",
    "  StructField('store', IntegerType()),\n",
    "  StructField('item', IntegerType()),\n",
    "  StructField('mae', FloatType()),\n",
    "  StructField('mse', FloatType()),\n",
    "  StructField('rmse', FloatType())\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T06:23:17.750474Z",
     "iopub.status.busy": "2022-01-30T06:23:17.749642Z",
     "iopub.status.idle": "2022-01-30T06:23:17.757780Z",
     "shell.execute_reply": "2022-01-30T06:23:17.756660Z",
     "shell.execute_reply.started": "2022-01-30T06:23:17.750437Z"
    }
   },
   "outputs": [],
   "source": [
    "# define udf to calculate metrics\n",
    "@pandas_udf( eval_schema, PandasUDFType.GROUPED_MAP )\n",
    "def evaluate_forecast( evaluation_pd ):\n",
    "\n",
    "    # get store & item in incoming data set\n",
    "    training_date = evaluation_pd['training_date'].iloc[0]\n",
    "    store = evaluation_pd['store'].iloc[0]\n",
    "    item = evaluation_pd['item'].iloc[0]\n",
    "\n",
    "    # calulate evaluation metrics\n",
    "    mae = mean_absolute_error( evaluation_pd['y'], evaluation_pd['yhat'] )\n",
    "    mse = mean_squared_error( evaluation_pd['y'], evaluation_pd['yhat'] )\n",
    "    rmse = sqrt( mse )\n",
    "\n",
    "    # assemble result set\n",
    "    results = {'training_date':[training_date], 'store':[store], 'item':[item], 'mae':[mae], 'mse':[mse], 'rmse':[rmse]}\n",
    "    return pd.DataFrame.from_dict( results )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T06:23:23.749964Z",
     "iopub.status.busy": "2022-01-30T06:23:23.749167Z",
     "iopub.status.idle": "2022-01-30T06:23:23.843531Z",
     "shell.execute_reply": "2022-01-30T06:23:23.842681Z",
     "shell.execute_reply.started": "2022-01-30T06:23:23.749920Z"
    }
   },
   "outputs": [],
   "source": [
    "# calculate metrics\n",
    "results = (\n",
    "  spark\n",
    "    .table('new_forecasts')\n",
    "    .filter('ds < \\'2018-01-01\\'') # limit evaluation to periods where we have historical data\n",
    "    .select('training_date', 'store', 'item', 'y', 'yhat')\n",
    "    .groupBy('training_date', 'store', 'item')\n",
    "    .apply(evaluate_forecast)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T06:23:24.566937Z",
     "iopub.status.busy": "2022-01-30T06:23:24.566598Z",
     "iopub.status.idle": "2022-01-30T06:23:24.572503Z",
     "shell.execute_reply": "2022-01-30T06:23:24.571558Z",
     "shell.execute_reply.started": "2022-01-30T06:23:24.566906Z"
    }
   },
   "outputs": [],
   "source": [
    "results.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Print The result of Forecasting** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**<br>\n",
    "The previous cell requires computing power if you don't have it like me run lower item-store combinations. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
